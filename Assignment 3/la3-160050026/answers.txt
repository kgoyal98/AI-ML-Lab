Observations:
For bagging, the test accuracy increases with the number of classifiers. For boosting, the test accuracy increases with the number of classifiers. For boosting, the test and validation accuracy does not increase much after 10 classifiers, although the training accuracy increases continuously. For both bagging and boosting, the training accuracy is more than the test accuracy by more than 10%. The training accuracy for bagging increased drastically initially with the increase in number of classifiers, but that of boosting increased slowly.

1.	The training accuracy of bagging for 20 classfiers is 92.2% and for boosting is 91.1%. Boosting usually involves reducing the bias or the training accuracy and bagging involves reducing the variance of the data. That is why, it can be seen in the plots that the training accuracy is greater for boosting for low values of number of classifiers but for around 15-20 classifiers onwards, the efficiency of bagging and boosting is nearly same.

2.	The statement is true. An ensemble combining perceptrons with weighted majority cannot be represented as an equivalent single perceptron. Although, we can represent the ensemble as a neural network by connecting the outputs of the base learners to another perceptron. This structure is not reducible to a single perceptron. If the ensemble could be represented as an perceptron, we could have achieved the training efficiency of an ensemble for a perceptron too. But we got ensemble efficiency to be greater than that of a perceptron. Also ensemble learning can classify non-linear separable data but a simple perceptron cannot.
